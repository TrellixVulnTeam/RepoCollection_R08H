{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    ":label:`chapter_rnn`\n",
    "\n",
    "\n",
    "So far we encountered two types of data: generic vectors and\n",
    "images. For the latter we designed specialized layers to take\n",
    "advantage of the regularity properties in them. In other words, if we\n",
    "were to permute the pixels in an image, it would be much more\n",
    "difficult to reason about its content of something that would look\n",
    "much like the background of a test pattern in the times of Analog TV.\n",
    "\n",
    "Most importantly, so far we tacitly assumed that our data is generated\n",
    "iid, i.e. independently and identically distributed, all drawn from some\n",
    "distribution. Unfortunately, this isn't true for most data. For\n",
    "instance, the words in this paragraph are written in sequence, and it\n",
    "would be quite difficult to decipher its meaning if they were\n",
    "permuted randomly. Likewise, image frames in a video, the audio signal\n",
    "in a conversation, or the browsing behavior on a website, all follow\n",
    "sequential order. It is thus only reasonable to assume that\n",
    "specialized models for such data will do better at describing it and\n",
    "at solving estimation problems.\n",
    "\n",
    "Another issue arises from the fact that we might not only receive a\n",
    "sequence as an input but rather might be expected to continue the\n",
    "sequence. For instance, the task could be to continue the series 2,\n",
    "4, 6, 8, 10, ... This is quite common in time series analysis, to\n",
    "predict the stock market, the fever curve of a patient or the\n",
    "acceleration needed for a race car. Again we want to have models that\n",
    "can handle such data.\n",
    "\n",
    "In short, while convolutional neural networks can efficiently process\n",
    "spatial information, recurrent neural networks are designed to better\n",
    "handle sequential information. These networks introduces state\n",
    "variables to store past information and, together with the current\n",
    "input, determine the current output.\n",
    "\n",
    "Many of the examples for using recurrent networks are based on text\n",
    "data. Hence, we will emphasize language models in this chapter. After\n",
    "a more formal review of sequence data we discuss basic concepts of a\n",
    "language model and use this discussion as the inspiration for the\n",
    "design of recurrent neural networks. Next, we describe the gradient\n",
    "calculation method in recurrent neural networks to explore problems\n",
    "that may be encountered in recurrent neural network training. For some\n",
    "of these problems, we can use gated recurrent neural networks, such as\n",
    "LSTMs and GRUs, described later in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```toc\n",
    ":maxdepth: 2\n",
    "\n",
    "sequence\n",
    "lang-model\n",
    "rnn\n",
    "lang-model-dataset\n",
    "rnn-scratch\n",
    "rnn-gluon\n",
    "bptt\n",
    "gru\n",
    "lstm\n",
    "deep-rnn\n",
    "bi-rnn\n",
    "machine-translation\n",
    "encoder-decoder\n",
    "seq2seq\n",
    "beam-search\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}