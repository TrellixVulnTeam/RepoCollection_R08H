{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence with Attention Mechanism\n",
    "\n",
    "In this section, we add the attention mechanism to the sequence to sequence\n",
    "model introduced in :numref:`chapter_seq2seq`\n",
    "to explicitly select state. The following figure shows the model\n",
    "architecture for a decoding time step. As can be seen, the memory of the\n",
    "attention layer consists of the encoder outputs of each time step. During\n",
    "decoding, the decoder output from the previous time step is used as the query,\n",
    "the attention output is then fed into the decoder with the input to provide\n",
    "attentional context information.\n",
    "\n",
    "![The second time step in decoding for the sequence to sequence model with attention mechanism.](../img/seq2seq_attention.svg)\n",
    "\n",
    "The layer structure in the encoder and the decoder is shown in the following figure.\n",
    "\n",
    "![](../img/seq2seq-attention-details.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import rnn, nn\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Now let's implement the decoder of this model. We add a MLP attention layer which has the same hidden size as the LSTM layer. The state passed from the encoder to the decoder contains three items:\n",
    "- the encoder outputs of all time steps, which are used as the attention layer's memory with identical keys and values\n",
    "- the hidden state of the last time step that is used to initialize the encoder's hidden state\n",
    "- valid lengths of the decoder inputs so the attention layer will not consider encoder outputs for padding tokens.\n",
    "\n",
    "In each time step of decoding, we use the output of the last RNN layer as the query for the attention layer. Its output is then concatenated with the input embedding vector to feed into the RNN layer. Despite the RNN layer hidden state also contains history information from decoder, the attention output explicitly selects the encoder outputs that are correlated to the query and suspends other non-correlated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionDecoder(d2l.Decoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n",
    "        self.attention_cell = d2l.MLPAttention(num_hiddens, dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = rnn.LSTM(num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Dense(vocab_size, flatten=False)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_len, *args):\n",
    "        outputs, hidden_state = enc_outputs\n",
    "        # Transpose outputs to (batch_size, seq_len, hidden_size)\n",
    "        return (outputs.swapaxes(0,1), hidden_state, enc_valid_len)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, hidden_state, enc_valid_len = state\n",
    "        X = self.embedding(X).swapaxes(0, 1)\n",
    "        outputs = []\n",
    "        for x in X:\n",
    "            # query shape: (batch_size, 1, hidden_size)\n",
    "            query = hidden_state[0][-1].expand_dims(axis=1)\n",
    "            # context has same shape as query\n",
    "            context = self.attention_cell(\n",
    "                query, enc_outputs, enc_outputs, enc_valid_len)\n",
    "            # concatenate on the feature dimension\n",
    "            x = nd.concat(context, x.expand_dims(axis=1), dim=-1)\n",
    "            # reshape x to (1, batch_size, embed_size+hidden_size)\n",
    "            out, hidden_state = self.rnn(x.swapaxes(0, 1), hidden_state)\n",
    "            outputs.append(out)\n",
    "        outputs = self.dense(nd.concat(*outputs, dim=0))\n",
    "        return outputs.swapaxes(0, 1), [enc_outputs, hidden_state,\n",
    "                                        enc_valid_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same hyper-parameters to create an encoder and decoder as the [\"Sequence to Sequence\"](../chapter_recurrent-neural-networks/seq2seq.md) section, we get the same decoder output shape, but the state structure is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 7, 10), 3, (4, 7, 16), 2, (2, 4, 16))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8,\n",
    "                             num_hiddens=16, num_layers=2)\n",
    "encoder.initialize()\n",
    "decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8,\n",
    "                                  num_hiddens=16, num_layers=2)\n",
    "decoder.initialize()\n",
    "X = nd.zeros((4, 7))\n",
    "state = decoder.init_state(encoder(X), None)\n",
    "out, state = decoder(X, state)\n",
    "out.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Again, we use the same training hyper-parameters as in\n",
    ":numref:`chapter_seq2seq`. The training loss is similar to the seq2seq model, because the\n",
    "sequences in the training dataset are relative short. The additional attention\n",
    "layer doesn't lead to a significant different. But due to both attention layer\n",
    "computational overhead and we unroll the time steps in the decoder, this model\n",
    "is much slower than the seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 0.114, time 34.6 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.065, time 34.2 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 150, loss 0.041, time 34.4 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200, loss 0.031, time 33.9 sec\n"
     ]
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0\n",
    "batch_size, num_examples, max_len = 64, 1e3, 10\n",
    "lr, num_epochs, ctx = 0.005, 200, d2l.try_gpu()\n",
    "\n",
    "src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(\n",
    "    batch_size, max_len, num_examples)\n",
    "encoder = d2l.Seq2SeqEncoder(\n",
    "    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqAttentionDecoder(\n",
    "    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "model = d2l.EncoderDecoder(encoder, decoder)\n",
    "d2l.train_ch7(model, train_iter, lr, num_epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we predict several sample examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go . => <unk> !\n",
      "Wow ! => <unk> !\n",
      "I'm OK . => <unk> <unk> ?\n",
      "I won ! => je l'ai emportÃ© !\n"
     ]
    }
   ],
   "source": [
    "for sentence in ['Go .', 'Wow !', \"I'm OK .\", 'I won !']:\n",
    "    print(sentence + ' => ' + d2l.translate_ch7(\n",
    "        model, sentence, src_vocab, tgt_vocab, max_len, ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}